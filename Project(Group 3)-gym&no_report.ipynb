{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f7a311f",
   "metadata": {},
   "source": [
    "@Time ：2023/7/7 \n",
    "\n",
    "@Auth ：**He Enhao, Bi Xiaoyang, Wang Qipeng, Li Haoyang**\n",
    "\n",
    "@File ：Play atari pong with reinforce algorithm by pytorch in **gym (PongNoFrameskip-v4)** platform.\n",
    "\n",
    "@IDE ：Jupyter\n",
    "\n",
    "@Environment: gym==0.21, python==3.8, pytorch==1.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e412416",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "\n",
    "    1. Introduction\n",
    "    2. Problem formulation \n",
    "    3. Data preparation\n",
    "    4. Processing pipeline \n",
    "    5. Experimental results and discussion\n",
    "    6. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6819984",
   "metadata": {},
   "source": [
    "In this project, we created AI agents in gym atari pong environment (PongNoFrameskip-v4) that produced the best action and used preprocessed pixels as features. We feed them into a deep Q-learning network. We implement it combined with a convolutional neural network (CNN). And we conducted some experiments, analysis the result, and compare the pong environment between the gym and gym-retro platform to test our model. We will discuss them in detail in this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08745cbf",
   "metadata": {},
   "source": [
    "# Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb0d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from life.utils.replay.replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced98d20",
   "metadata": {},
   "source": [
    "# Environment Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53fcaa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "def make_env(env, stack_frames=True, episodic_life=True, clip_rewards=False, scale=False):\n",
    "    if episodic_life:\n",
    "        env = EpisodicLifeEnv(env)\n",
    "\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
    "        env = FireResetEnv(env)\n",
    "\n",
    "    env = WarpFrame(env)\n",
    "    if stack_frames:\n",
    "        env = FrameStack(env, 4)\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "class RewardScaler(gym.RewardWrapper):\n",
    "\n",
    "    def reward(self, reward):\n",
    "        return reward * 0.1\n",
    "\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "\n",
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not believe how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=2)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k),\n",
    "                                                dtype=env.observation_space.dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "                                                shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "        \"\"\"\n",
    "        super(EpisodicLifeEnv, self).__init__(env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "        self.was_real_reset = False\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives and lives > 0:\n",
    "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset()\n",
    "            self.was_real_reset = True\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "            self.was_real_reset = False\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, noop_max=30):\n",
    "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
    "        No-op is assumed to be action 0.\n",
    "        \"\"\"\n",
    "        super(NoopResetEnv, self).__init__(env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
    "        self.env.reset()\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = np.random.randint(1, self.noop_max + 1)\n",
    "        assert noops > 0\n",
    "        obs = None\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(0)\n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "        return obs\n",
    "\n",
    "def get_state(obs):\n",
    "    state = np.array(obs)\n",
    "    state = state.transpose((2, 0, 1))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16983e0",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1bcf779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render avi or gif\n",
    "def renderFrames(frame_array, savePath, fileName, fps, otype='AVI'):\n",
    "    print('Creating replay ...', end=' ')\n",
    "    if otype == 'AVI':\n",
    "        fileName += '.avi'\n",
    "        height, width, layers = frame_array[0].shape\n",
    "        if layers == 1:\n",
    "            layers = 0\n",
    "        size = (width, height)\n",
    "        out = cv2.VideoWriter(savePath + fileName, cv2.VideoWriter_fourcc(*'DIVX'), fps, size, layers)\n",
    "        for i in range(len(frame_array)):\n",
    "            out.write(frame_array[i])\n",
    "        out.release()\n",
    "        print('Done. Saved to {}'.format(savePath + fileName))\n",
    "    else:\n",
    "        print('Error: Invalid type, must be AVI.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88827968",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "## Build DQN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ff26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    ''' DQN Algorithm '''\n",
    "\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma,\n",
    "                 epsilon, target_update, device, q_net):\n",
    "        self.action_dim = action_dim\n",
    "        self.q_net = q_net.to(device)\n",
    "        # Target network\n",
    "        self.target_q_net = q_net.to(device)\n",
    "        # using ADAM optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(),\n",
    "                                          lr=learning_rate)\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = epsilon  # epsilon-greedy strategy\n",
    "        self.target_update = target_update  # update frequency\n",
    "        self.count = 0  #counter\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):  # epsilon-greedy strategy for action choosing\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "            action = self.q_net(state).argmax().item()\n",
    "        return action\n",
    "\n",
    "    def max_q_value(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        return self.q_net(state).max().item()\n",
    "\n",
    "    def save_model(self,save_path):\n",
    "        print('Saving models ...', end=' ')\n",
    "        torch.save(self.q_net.state_dict(),save_path + 'DQN_eval.pth')\n",
    "        torch.save(self.target_q_net.state_dict(),save_path + 'DQN_next.pth')\n",
    "        print('Done.')\n",
    "\n",
    "    def load_model(self,load_path):\n",
    "        print('Loading models ...', end=' ')\n",
    "        self.q_net.load_state_dict(torch.load(load_path+'Pong_gym_eval.pth'))\n",
    "        self.target_q_net.load_state_dict(torch.load(load_path+'Pong_gym_next.pth'))\n",
    "        print('Done.')\n",
    "\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)  # Q value\n",
    "        # maximum Q value in next state\n",
    "        max_next_q_values = self.target_q_net(next_states).max(1)[0].view(\n",
    "            -1, 1)\n",
    "        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones\n",
    "                                                                )  # TD\n",
    "        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  # MSE\n",
    "        self.optimizer.zero_grad()  # the gradient accumulates by default by Pytorch, we manually set this as 0\n",
    "        dqn_loss.backward()  #backward update\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.count % self.target_update == 0:\n",
    "            self.target_q_net.load_state_dict(\n",
    "                self.q_net.state_dict())  # update strategy\n",
    "        self.count += 1\n",
    "        \n",
    "\n",
    "    def play_game(self, env):\n",
    "        print('Playing game ...', end=' ')\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "        observation = get_state(observation)\n",
    "        done = False\n",
    "        steps = 0\n",
    "        frames = []\n",
    "        obsFrames = []\n",
    "        while not done:\n",
    "            action = self.take_action(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            observation = get_state(observation)\n",
    "            score += reward\n",
    "            steps += 1\n",
    "            frames.append(env.render(mode='rgb_array'))\n",
    "            obsFrames.append(observation)\n",
    "        print('Done. Score: {}'.format(score))\n",
    "        return frames, obsFrames\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=4, n_actions=14):\n",
    "        \"\"\"\n",
    "        Initialize Deep Q Network\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_actions (int): number of outputs\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.head = nn.Linear(512, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() / 255\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11786d90",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0ae74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent, env, replay_buffer, minimal_size, batch_size, num_episodes=500,\n",
    "              conti_action=False, return_agent=False,load=False,load_path=None,save=True,save_path=None):\n",
    "    \"\"\"\n",
    "    :param agent:\n",
    "    :param env:\n",
    "    :param num_episodes:\n",
    "    :param replay_buffer:\n",
    "    :param minimal_size: start training only when replay_buffer is larger than minimal_size\n",
    "    :param batch_size:\n",
    "    :param conti_action: if can be appied for continious action\n",
    "    :param return_agent:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if load:\n",
    "        agent.load_model(load_path)\n",
    "\n",
    "    return_list = []\n",
    "    for i in range(10):\n",
    "        with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(int(num_episodes / 10)):\n",
    "                episode_return = 0\n",
    "                state = env.reset()\n",
    "                state = get_state(state)  # dimension trans\n",
    "                done = False\n",
    "                while not done:\n",
    "                    action = agent.take_action(state)\n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "                    next_state = get_state(next_state)  # dimension trans\n",
    "                    replay_buffer.add(state, action, reward, next_state, done)\n",
    "                    state = next_state\n",
    "                    episode_return += reward\n",
    "                    # start training only when replay_buffer is larger than minimal_size\n",
    "                    if replay_buffer.size() > minimal_size:\n",
    "                        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                        transition_dict = {\n",
    "                            'states': b_s,\n",
    "                            'actions': b_a,\n",
    "                            'next_states': b_ns,\n",
    "                            'rewards': b_r,\n",
    "                            'dones': b_d\n",
    "                        }\n",
    "                        agent.update(transition_dict)\n",
    "                return_list.append(episode_return)\n",
    "                if (i_episode + 1) % 10 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'episode':\n",
    "                            '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                        'return':\n",
    "                            '%.3f' % np.mean(return_list[-10:])\n",
    "                    })\n",
    "                pbar.update(1)\n",
    "\n",
    "    if save:\n",
    "        agent.save_model(save_path)\n",
    "        \n",
    "    if return_agent:\n",
    "        return return_list, agent\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8ead6",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92fb9a2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-4f60e0890c53>:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
      "Iteration 0: 100%|██████████| 2/2 [02:15<00:00, 67.74s/it]\n",
      "Iteration 1: 100%|██████████| 2/2 [02:08<00:00, 64.05s/it]\n",
      "Iteration 2: 100%|██████████| 2/2 [01:37<00:00, 48.75s/it]\n",
      "Iteration 3: 100%|██████████| 2/2 [01:43<00:00, 51.94s/it]\n",
      "Iteration 4: 100%|██████████| 2/2 [01:21<00:00, 40.62s/it]\n",
      "Iteration 5: 100%|██████████| 2/2 [02:15<00:00, 67.54s/it]\n",
      "Iteration 6: 100%|██████████| 2/2 [01:43<00:00, 51.97s/it]\n",
      "Iteration 7: 100%|██████████| 2/2 [02:11<00:00, 65.85s/it]\n",
      "Iteration 8: 100%|██████████| 2/2 [02:48<00:00, 84.18s/it]\n",
      "Iteration 9: 100%|██████████| 2/2 [02:03<00:00, 61.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.0, 16.0, 18.0, 17.0, 18.0, 18.0, -4.0, -15.0, 18.0, 20.0, -5.0, 6.0, 20.0, 3.0, 7.0, -7.0, 4.0, -3.0, -5.0, 10.0]\n",
      "Training finished,using 1211.2879800796509 min.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = make_env(env)\n",
    "state_dim = env.observation_space.shape\n",
    "action_dim = env.action_space.shape or env.action_space.n\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "load_path = './models/'\n",
    "save_path = 'C:/Users/86139/Desktop/Project/rl-pong-main/models/'\n",
    "\n",
    "base_net = CNN(\n",
    "    in_channels=4,\n",
    "    n_actions=6\n",
    ")\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "agent_dqn = DQN(\n",
    "    state_dim=state_dim,\n",
    "    hidden_dim=128,\n",
    "    action_dim=action_dim,\n",
    "    learning_rate=0.0001,  # reducing the learning rate\n",
    "    gamma=0.92,\n",
    "    epsilon=0.01,\n",
    "    target_update=50,  # increase to 50 from 10\n",
    "    device=device,\n",
    "    q_net=base_net,\n",
    ")\n",
    "start_t = time.time()\n",
    "result, agent = train_dqn(\n",
    "    agent=agent_dqn,\n",
    "    env=env,\n",
    "    replay_buffer=replay_buffer,\n",
    "    minimal_size=500,\n",
    "    num_episodes=20, # 20 episodes for showing the process. We trained the agent for 1000 episodes. \n",
    "    batch_size=64,  # batch_size reduced from 128 to 64, but the speed of training on GPU increases\n",
    "    return_agent=True,\n",
    "    load=True,\n",
    "    save=False,\n",
    "    load_path=load_path # load the network parameters of 1000 episodes \n",
    ")\n",
    "\n",
    "\n",
    "print(result)\n",
    "print(\"Training finished,using {} min.\".format(time.time() - start_t))\n",
    "\n",
    "# joblib.dump(result, \"./result/dqn1000iter_result_list.dat\")\n",
    "# joblib.dump(agent, \"./result/dqn1000iter_agent.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d51f3ff",
   "metadata": {},
   "source": [
    "# Play&Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed1b65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing game ... Done. Score: 21.0\n",
      "Creating replay ... Done. Saved to ./Pong_gym_playgame.avi\n"
     ]
    }
   ],
   "source": [
    "frames, obsFrames = agent_dqn.play_game(env)\n",
    "fileName = 'Pong_gym_playgame'\n",
    "renderFrames(frames, './', fileName, 60, otype='AVI')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
